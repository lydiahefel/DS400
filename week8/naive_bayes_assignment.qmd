---
title: "Naive Bayes Classification Assignment"
format: html
editor: visual
---

## Assignment Description

This assignment is designed to test your knowledge of Naive Bayes Classification. It closely mirrors our [naive_bayes_penguins.qmd](https://github.com/NSF-ALL-SPICE-Alliance/DS400/blob/main/week7/naive_bayes_penguins.qmd) from lectures 10/1 and 10/3. We reflect back on the true vs fake news dataset from the beginning of the semester and apply the new skills in our bayesian toolbox.

This assignment is worth 16 points and is due by 10:00am on October 15th. Each section has a number of points noted. To turn in this assignment, render this qmd and save it as a pdf, it should look beautiful. If you do not want warning messages and other content in the rendered pdf, you can use `message = FALSE, warning = FALSE` at the top of each code chunk as it appears in the libraries code chunk below.

### Load Libraries

```{r, message=FALSE, warning=FALSE}
library(bayesrules)
library(tidyverse)
library(e1071)
library(janitor)
library(plotly)
```

### Read in data

```{r}
data(fake_news)
```

### Challenge

[**Exercise 14.7**](https://www.bayesrulesbook.com/chapter-14#exercises-13) **Fake news: three predictors**

Suppose a ***new news article*** is posted online – it has a 15-word title, 6% of its words have negative associations, and its title *doesn’t* have an exclamation point. We want to know if it is fake or real

### Visualization (Exploratory Data Analysis) - 2 points

Below, insert a code chunk(s) and use `ggplot` to visualize the features of the data we are interested in. This can be one or multiple visualizations

-   Type (fake vs real)

-   Number of words in the title (numeric value)

-   Negative associations (numeric value)

-   Exclamation point in the title (true vs false)

```{r}
fake_news_vis <- ggplot(data = fake_news, aes(x = title_has_excl, fill = type)) +
  geom_bar()

ggplotly(fake_news_vis)
```

```{r}
fake_news_vis2 <- ggplot(fake_news, aes(x = negative, fill = type)) +
  geom_histogram()

ggplotly(fake_news_vis2)
```

```{r}
fake_news_vis3 <- ggplot(fake_news, aes(x = title_words, fill = type)) +
  geom_histogram()

ggplotly(fake_news_vis3)
```

```{r}
fake_news_vis4 <- ggplot(fake_news, aes(x = title_words, y = negative, color = type)) +
  geom_point()

ggplotly(fake_news_vis4)
```

### Interpretation of Visualization - 2 points

Below, write a few sentences explaining whether or not this ***new news article*** is true or fake solely using your visualization above

-   The visualizations above break down each variable of the new news article.
-   Based on the first visualization, 88/132 of the articles without an exclamation point in the title were real, which means the new news article is more likely real, based on that variable alone.
-   Based on the second visualization, only 1/3 of the articles with roughly 6% of words with negative associations were real, which means the new news article is more likely fake, based on that variable alone.
-   Based on the third visualization, only 9/21 of the articles with 15 word titles were real, which means the new news article is more likely fake, based on that variable alone.
-   The visualizations together seem to lean more towards the new news article being fake rather than real, but it is hard to commit to a definite answer based solely on the visualizations.

### Perform Naive Bayes Classification - 3 points

Based on these three features (15-word title, 6% of its words have negative associations, and its title *doesn’t* have an exclamation point), utilize naive Bayes classification to calculate the posterior probability that the article is real. Do so using `naiveBayes()` with `predict()`.

Below, insert the code chunks and highlight your answer

```{r}
#Create prediction model
fake_news_prediction_mod <- naiveBayes(type ~ title_has_excl + negative + title_words, data = fake_news)
```

```{r}
#Create new news article data
new_news_article <- data.frame(title_has_excl = FALSE, negative = 6, title_words = 15)
```

```{r}
#Use prediction model on new news article data
predict(fake_news_prediction_mod, newdata = new_news_article, type = "raw")
```

```{r}
#Use prediction model on fake news dataset
fake_news <- fake_news %>% 
  mutate(predicted_type = predict(fake_news_prediction_mod, newdata = .))

#Create confusion matrix
fake_news %>% 
  tabyl(type, predicted_type) %>% 
  adorn_percentages("row") %>% 
  adorn_pct_formatting(digits = 2) %>% 
  adorn_ns
```

-   **Calculated Posterior Probability:** 0.1224221

-   In the code chunks above, a prediction model was created to identify whether news articles from the fake_news dataset are real or fake. A data frame based solely on the data of the new news article was created, the prediction model was then applied to said data. The calculated posterior probability that the new news article is real was **0.1224221**, based on the prediction model. A confusion matrix was created based on the prediction model to see how accurate the model may have predicted the type of the new news article. The accuracy was quite low, recognizing real articles as real 87.78% of the time, and recognizing fake articles as fake 48.33% of time.

### Break Down the Model - 5 points

Similar to the penguins example, we are going to break down the model we created above. To do this we need to find:

-   Probability(15 - word title\| article is real) using `dnorm()`

-   Probability(6% of words have negative associations \| article is real) using `dnorm()`

-   Probability(no exclamation point in title \| article is real)

    -   Multiply these probabilities and save as the object **`probs_real`**

-   Probability(15 - word title\| article is fake) using `dnorm()`

-   Probability(6% of words have negative associations \| article is fake) using `dnorm()`

-   Probability(no exclamation point in title \| article is fake)

    -   Multiply these probabilities and save as the object **`probs_fake`**

Lastly divide your **`probs_real`** by the sum of **`probs_real`** and **`probs_fake`** to see if you can reproduce the output from `naiveBayes()` above

```{r}
#Probability(15 word title | article is real)
prob_real_title_words <- dnorm(15, mean = 10.42222, sd = 3.204554)

#Probability(6% of words have negative associations | article is real)
prob_real_negative <- dnorm(6, mean = 2.806556, sd = 1.190917)

#Probability(no exclamation point in title | article is real)
prob_real_title_no_excl <- 0.97777778

#Multiply these probabilities and save as the object probs_real
probs_real <- (prob_real_title_words * prob_real_negative * prob_real_title_no_excl)

probs_real
```

```{r}
#Probability(15 - word title| article is fake)
prob_fake_title_words <- dnorm(15, mean = 12.31667, sd = 3.743884)

#Probability(6% of words have negative associations | article is fake)
prob_fake_negative <- dnorm(6, mean = 3.606333, sd = 1.466429)

#Probability(no exclamation point in title | article is fake)
prob_fake_title_no_excl <- 0.73333333

#Multiply these probabilities and save as the object probs_fake
probs_fake <- (prob_fake_title_words * prob_fake_negative * prob_fake_title_no_excl)

probs_fake
```

```{r}
probs_real / (probs_real + probs_fake)
```

### Confusion Matrix - 2 points

Calculate a confusion matrix by first mutating a column to fake_news called `predicted_type` . Then, use `tabyl()` to create the matrix

```{r}
fake_news <- fake_news %>% 
  mutate(predicted_type = predict(fake_news_prediction_mod, newdata = .))

fake_news %>% 
  tabyl(type, predicted_type) %>% 
  adorn_percentages("row") %>% 
  adorn_pct_formatting(digits = 2) %>% 
  adorn_ns
```

### How can our model be improved? - 2 points

Think about the results of the confusion matrix, is the model performing well? Try creating a new model that uses all of the features in the fake_news dataset to make a prediction on type (fake vs true). Then, create a new confusion matrix to see if the model improves.

-   Based on the results of the confusion matrix, the model is not performing well.

```{r}
#Create new model
updated_fake_news_prediction_mod <- naiveBayes(type ~., data = fake_news)

#Use new model on fake news dataset
fake_news <- fake_news %>% 
  mutate(predicted_type = predict(updated_fake_news_prediction_mod, newdata = .))

#Create new confusion matrix
fake_news %>% 
  tabyl(type, predicted_type) %>% 
  adorn_percentages("row") %>% 
  adorn_pct_formatting(digits = 2) %>% 
  adorn_ns
```

-   The use of all features in the fake_new dataset improved the predictive abilities of the model.
